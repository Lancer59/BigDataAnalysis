{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLUKdDJeOXbzevxWZfP3wf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lancer59/BigDataAnalysis/blob/main/Assignment_(RDD%2C_DataFrame%2C_ML).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1 on RDD**"
      ],
      "metadata": {
        "id": "dDRnhABpTA3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "z5OEjnJ_Nyfn",
        "outputId": "8a979528-678f-4e14-c542-69903f8be0f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 50 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 60.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=9490a027d8976f86dd5883f4a106ac80ca386ad536590d7430ff3f2d6547d23b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#store file as rdd.\n",
        "from pyspark import SparkContext\n",
        "sc=SparkContext.getOrCreate()\n",
        "rdd=sc.textFile('/content/lvl.txt')\n",
        "\n",
        "#Remove blankspaces.\n",
        "srdd=rdd.filter(lambda l:l.strip())\n",
        "\n",
        "#Split into words.\n",
        "wrdd=srdd.flatMap(lambda l:l.split(\" \"))\n",
        "\n",
        "#Remove stopwords.\n",
        "stopwords=[\"in\",\"on\",\"for\",\"the\",\"a\"]\n",
        "stpwrd=wrdd.filter(lambda w:w not in stopwords)\n",
        "\n",
        "\n",
        "#Remove punctuations.\n",
        "import string\n",
        "rrdd=stpwrd.map(lambda w:w.translate(str.maketrans('','',string.punctuation)))\n",
        "\n",
        "##(i) Word with highest number of occurrence.\n",
        "grdd=wrdd.groupBy(lambda w:w[:])\n",
        "mrdd=grdd.map(lambda x:(len(x[1]),x[0]))\n",
        "srdd=mrdd.sortByKey(False)\n",
        "hnw=srdd.take(2)[0][1]\n",
        "print(\"\\n(i) The word with highest number of occurrence = \"+str(hnw))\n",
        "\n",
        "##(ii) Character with highest number of occurrence.\n",
        "grdd=wrdd.groupBy(lambda w:w[0:1])\n",
        "mrdd=grdd.map(lambda x:(len(x[1]),x[0]))\n",
        "srdd=mrdd.sortByKey(False)\n",
        "hnc=srdd.take(2)[0][1]\n",
        "print(\"\\n(ii) The character with highest number of occurrence = \"+str(hnc))\n",
        "\n",
        "##(iii) Compute occurrence of any given word accepted from keyboard.\n",
        "we=input(\"\\n(iii) Enter the word of whose occurrence must be returned: \")\n",
        "grdd=wrdd.groupBy(lambda w:w[:])\n",
        "mrdd=grdd.map(lambda x:(len(x[1]),x[0]))\n",
        "swrdd=mrdd.filter(lambda w:w[1] == we)\n",
        "wno=swrdd.take(1)[0][0]\n",
        "print(\"      The occurrence of the entered word is = \"+str(wno))\n",
        "\n",
        "##(iv) Line sorted in descending order of number of words they contain.\n",
        "mrdd=rdd.map(lambda x:(len(x.split(\" \")),x))\n",
        "srdd=mrdd.sortByKey(False)\n",
        "wind=list(srdd.take(srdd.count()))\n",
        "print(\"\\n(iv) The Lines sorted in descending order of number of words they contain: \\n\")\n",
        "for i in wind:\n",
        "  print(i[1])\n",
        "\n",
        "##(v) Take 80% sample twice and perform intersection between them and check number of lines common.\n",
        "srdd1=rrdd.sample(False, 0.8, 10)\n",
        "srdd2=rrdd.sample(False, 0.8, 90)\n",
        "isample=srdd1.intersection(srdd2)\n",
        "print(\"\\n (v)\")\n",
        "print(\" The number of lines in sample 1: \"+str(srdd1.count()))\n",
        "print(\" The number of lines in sample 2: \"+str(srdd2.count()))\n",
        "print(\" The number of lnes which is common: \"+str(isample.count()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrWYwLalTYet",
        "outputId": "9535fa87-7fb8-493b-8377-40a08bffbfb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(i) The word with highest number of occurrence = I\n",
            "\n",
            "(ii) The character with highest number of occurrence = a\n",
            "\n",
            "(iii) Enter the word of whose occurrence must be returned: the\n",
            "      The occurrence of the entered word is = 16\n",
            "\n",
            "(iv) The Lines sorted in descending order of number of words they contain: \n",
            "\n",
            "I love you so much and I am so glad I met you. You are the one I have been waiting for all of my life. The little things, like making sure to let me know when I look beautiful and holding my hand whenever you can, makes me feel so loved. You are the love of my life. Without you I am incomplete. I love you more than any woman I have ever met or ever will meet in the future. You are amazing\n",
            "Every time I look at you, my heart skips a beat. When I feel your hugs, my strength becomes tenfold. I cannot imagine a day without you — and am certain I don’t want to know what that feels like… I love you more than words can even say; you are the light of my life, my soulmate, and my best friend.\n",
            "You are the love of my life; you are my world. I would do anything to make you happy. I want to see your beautiful smile every moment of every day. You bring me joy and happiness, and I promise to give you all of mine. You mean the world to me, and I will forever love you.\n",
            "I love you and have never been so sure of anything in my life. I could get lost in your eyes forever. Whenever I look into them they make me melt, I smile and laugh every time you kiss my neck. Your lips fit with mine perfectly like two pieces of a puzzle coming together.\n",
            "So I guess what I’m trying to say is that I love you. So much. You mean the world to me and I wouldn’t trade you for anything. You are my heart, my soul, and in a million years there will never be anyone else for me but you.\n",
            "It only grew stronger by the day. But as always I didn’t listen to the world, the people around me. If only we could have been brave enough and stuck together. So this is what it means to be without you all these years…\n",
            "You look so fine, your skin so soft. I love seeing you every day. I get butterflies in my stomach anytime we’re this close. I’ve liked you for a while — you’re the one I want to be with forever and ever.\n",
            "You are my everything. I love you more than anything in the world and am so lucky to have you by my side. You mean the world to me and I hope to spend every day making you happy!\n",
            "I think it’s going pretty well. I don’t know of any couples that don’t run into their share of issues along the way, but as far as I can tell, we do ok.\n",
            "Love is a word that shouldn’t be introduced when I am around you because it’s just not needed. It is so apparent that we are meant to be, it’s almost obnoxiously obvious.\n",
            "Every single day, I fall even more in love with you. Your smile melts my heart and your eyes send so many different emotions through me. I can’t imagine life without you.\n",
            "We’ve only been together four months and I already know you are the one. You bring so much happiness into my life, I can’t imagine being without you! I love you!\n",
            "I’d like to share with you what my life was like before I met you. It was boring, old, and meaningless. But now it’s spectacular, new, and meaningful. I LOVE YOU!\n",
            "I love you so much that I would even go as far as lip glossing one of your bras! And someday, if you’re lucky, maybe I’ll even wear it.\n",
            "I hope time and erase everything that you’re feeling bad about. I love to be around you because you are so beautiful! trust me, I am telling the truth.\n",
            "I love you. I’m not saying it because you want me to, or because I have to, or because it’s Valentines day, but because it’s true.\n",
            "Let’s make our relationship status official in case there are any holdouts. We’re dating an\n",
            "\n",
            "\n",
            "\n",
            " (v)\n",
            " The number of lines in sample 1: 532\n",
            " The number of lines in sample 2: 531\n",
            " The number of lnes which is common: 207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2 on DataFrames**"
      ],
      "metadata": {
        "id": "Xc3XXTRA1fX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataframe construction\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "sess=SparkSession.builder.getOrCreate()\n",
        "df=sess.read.csv('/content/cars.csv',header=True) \n",
        "\n",
        "##(i) Find the number of records whose income > 8000\n",
        "A1=df.filter(df.income>8000).count()\n",
        "print(\"\\n (i) The number of records whose income > 8000 is = \"+str(A1))\n",
        "\n",
        "##(ii) Find the number of records whose income <4000 and sales>1000\n",
        "A21=df.filter(df.income<4000)\n",
        "A2=A21.filter(df.sales>1000).count()\n",
        "print(\"\\n (ii) The number of records whose income <4000 and sales>1000 is = \"+str(A2))\n",
        "\n",
        "##(iii) Find the number of records for different ages and order them in descending order\n",
        "A31=df.select('age').distinct()\n",
        "print(\"\\n (iii) The number of records for different ages is = \"+str(A31.count())+\"\\n The order:\")\n",
        "A3=df.select('age').groupBy('age').count().orderBy(col('count').desc())\n",
        "print(str(A3.show()))\n",
        "\n",
        "##(iv) Find mean sales of male(1) and female (0)\n",
        "A4=df.groupBy('gender').agg({'sales':'Mean'})\n",
        "print(\"\\n (iv) Mean sales of male(1) and female(0) are: \")\n",
        "A4.show()\n",
        "\n",
        "##(v) Find the descriptive statistics of debt\n",
        "A5=df.select('debt').describe()\n",
        "print(\"\\n (v) descriptive statistics of debt: \")\n",
        "A5.show()\n",
        "\n",
        "\n",
        "##(vi) Find correlation between miles and debt\n",
        "import pyspark.sql.types as typ\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark import SparkContext,SQLContext\n",
        "labels=[\n",
        "    ('age',typ.IntegerType()),\n",
        "    ('gender',typ.IntegerType()),\n",
        "    ('miles',typ.IntegerType()),\n",
        "    ('debt',typ.IntegerType()),\n",
        "    ('income',typ.IntegerType()),\n",
        "    ('sales',typ.IntegerType())\n",
        "]\n",
        "schema = typ.StructType([\n",
        "typ.StructField(e[0], e[1], False) for e in labels\n",
        "])\n",
        "sess=SparkSession.builder.getOrCreate()\n",
        "df=sess.read.csv('/content/cars.csv',header=True,schema=schema) \n",
        "print(\"\\n (vi) Correlation between miles and debt: \")\n",
        "A6=df.stat.corr('miles','debt')\n",
        "print(str(A6))\n",
        "\n",
        "##(vii)Convert the dataframe to table and write an SQL query for finding\n",
        "       #grouping of age,gender and number of records of each group\n",
        "print(\"\\n (vii) Age,gender and number of records of each group: \")\n",
        "df.createOrReplaceTempView('cars_table')\n",
        "sq=SQLContext(sc)\n",
        "sq.sql('select age,gender,count(*) from cars_table group by age,gender ').show()\n",
        "\n",
        "###(viii)Store the results of question (iii) in a csv called A8.csv\n",
        "print(\"\\n (viii) Results of question (iii) in a csv called A8! \")\n",
        "A8=df.groupBy('age').count().orderBy(col('count').desc())\n",
        "A8.write.format(\"csv\").save('/content/A8.csv')\n"
      ],
      "metadata": {
        "id": "f85AITkb1qDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1488a9d1-7906-40a7-f637-7ffdd3b3f779"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " (i) The number of records whose income > 8000 is = 295\n",
            "\n",
            " (ii) The number of records whose income <4000 and sales>1000 is = 235\n",
            "\n",
            " (iii) The number of records for different ages is = 42\n",
            " The order:\n",
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 25|   41|\n",
            "| 30|   34|\n",
            "| 27|   33|\n",
            "| 24|   32|\n",
            "| 37|   32|\n",
            "| 36|   30|\n",
            "| 28|   30|\n",
            "| 29|   29|\n",
            "| 21|   29|\n",
            "| 26|   29|\n",
            "| 20|   28|\n",
            "| 22|   28|\n",
            "| 23|   25|\n",
            "| 49|   24|\n",
            "| 40|   24|\n",
            "| 45|   24|\n",
            "| 46|   23|\n",
            "| 58|   23|\n",
            "| 48|   23|\n",
            "| 59|   23|\n",
            "+---+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n",
            "\n",
            " (iv) Mean sales of male(1) and female(0) are: \n",
            "+------+------------------+\n",
            "|gender|        avg(sales)|\n",
            "+------+------------------+\n",
            "|     0|12024.953091684434|\n",
            "|     1|11371.726720647774|\n",
            "+------+------------------+\n",
            "\n",
            "\n",
            " (v) descriptive statistics of debt: \n",
            "+-------+------------------+\n",
            "|summary|              debt|\n",
            "+-------+------------------+\n",
            "|  count|               963|\n",
            "|   mean|14109.004153686397|\n",
            "| stddev| 18273.70248104874|\n",
            "|    min|                 0|\n",
            "|    max|              9991|\n",
            "+-------+------------------+\n",
            "\n",
            "\n",
            " (vi) Correlation between miles and debt: \n",
            "0.5447908747188632\n",
            "\n",
            " (vii) Age,gender and number of records of each group: \n",
            "+---+------+--------+\n",
            "|age|gender|count(1)|\n",
            "+---+------+--------+\n",
            "| 30|     0|      14|\n",
            "| 58|     1|      12|\n",
            "| 48|     1|       9|\n",
            "| 54|     0|       3|\n",
            "| 56|     0|       7|\n",
            "| 25|     1|      20|\n",
            "| 42|     0|       4|\n",
            "| 41|     0|       7|\n",
            "| 29|     0|      13|\n",
            "| 36|     1|      10|\n",
            "| 59|     0|      10|\n",
            "| 23|     0|      10|\n",
            "| 41|     1|       5|\n",
            "| 26|     0|      16|\n",
            "| 39|     0|       6|\n",
            "| 28|     1|      16|\n",
            "| 26|     1|      13|\n",
            "| 31|     1|       7|\n",
            "| 59|     1|      13|\n",
            "| 46|     0|       9|\n",
            "+---+------+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            " (viii) Results of question (iii) in a csv called A8! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3 on Machine Learning**"
      ],
      "metadata": {
        "id": "3RzrlU2kb2NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.numeric import array_equiv\n",
        "##(i) Data Preprocessing\n",
        "from pyspark.sql.session import SparkSession\n",
        "sess=SparkSession.builder.getOrCreate()\n",
        "import pyspark.sql.types as typ\n",
        "labels=[('PM25',typ.DoubleType()),\n",
        "        ('PM10',typ.DoubleType()),\n",
        "        ('NO',typ.DoubleType()),\n",
        "        ('NO2',typ.DoubleType()),\n",
        "        ('NOX',typ.DoubleType()),\n",
        "        ('NH3',typ.DoubleType()),\n",
        "        ('CO',typ.DoubleType()),\n",
        "        ('SO2',typ.DoubleType()),\n",
        "        ('O3',typ.DoubleType()),\n",
        "        ('Benzene',typ.DoubleType()),\n",
        "        ('Toluene',typ.DoubleType()),\n",
        "        ('Xylene',typ.DoubleType()),\n",
        "        ('AQI',typ.DoubleType()),\n",
        "       \n",
        "        ]\n",
        "schema = typ.StructType([\n",
        "typ.StructField(e[0], e[1], False) for e in labels\n",
        "])\n",
        "aq=sess.read.csv(\"/content/airqualitydataset.csv\",header=True,schema=schema)\n",
        "aq.dropDuplicates()\n",
        "aq.dropna()\n",
        "\n",
        "##(ii) Filter our important columns contributing to AQI\n",
        "import pyspark.mllib.stat as st \n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "rdd = aq.rdd \n",
        "rdd.take(2)\n",
        "rdd=rdd.map(lambda row: [e for e in row])\n",
        "rdd.take(2)\n",
        "import numpy as np\n",
        "sta = st.Statistics.colStats(rdd)\n",
        "sta.mean(),np.sqrt(sta.variance())\n",
        "print(\"\\n Correlation corresponding to last AQI column: \")\n",
        "corrs = st.Statistics.corr(rdd) \n",
        "print(str(corrs[-1]))\n",
        "print(\"\\n AirQuality columns: \")\n",
        "print(str(aq.columns))\n",
        "aq=aq.select('PM25','PM10','NO2','NOX','SO2','aqi')\n",
        "vectorAssembler = VectorAssembler(inputCols = [ 'PM25','PM10','NO2','NOX','SO2',], outputCol = 'features')\n",
        "aqv = vectorAssembler.transform(aq)\n",
        "print(\"\\n The filtered features and aqi columns: \" )\n",
        "aqv.select('features','aqi').show()\n",
        "\n",
        "##(iii) Split into 70, 30 train and test dataframes\n",
        "aqs = aqv.randomSplit([0.7, 0.3])\n",
        "train=aqs[0]\n",
        "test=aqs[1]\n",
        "print(\"\\n The Train and Test count respectively \"+str(train.count())+\" \"+str(test.count()))\n",
        "\n",
        "##(iv) Apply Linear Regression, Decision Tree classifier and Gradient Boost\n",
        "       #models on train data frames\n",
        "\n",
        "#Linear regression\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(featuresCol = 'features', labelCol='aqi', maxIter=10, regParam=0.3, elasticNetParam=0.8)#balance between overfitting and underfitting\n",
        "lrm = lr.fit(train)\n",
        "#Weights\n",
        "print(\"\\n Coefficients: \" + str(lrm.coefficients))\n",
        "#Bias\n",
        "print(\" Intercept: \" + str(lrm.intercept))\n",
        "print()\n",
        "#Decision Tree Classifier\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "\n",
        "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'aqi')\n",
        "dtm = dt.fit(train)\n",
        "\n",
        "#Gradient Booster\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'aqi', maxIter=10)\n",
        "gbtm = gbt.fit(train)\n",
        "\n",
        "##(v)For each model find predictions on test dataframes and evaluate\n",
        "    #prediction accuracy.\n",
        "\n",
        "#Linear Regression\n",
        "lrp = lrm.transform(test)\n",
        "lrp.select(\"prediction\",\"aqi\",\"features\").show(5)\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "lre = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
        "                 labelCol=\"aqi\",metricName=\"r2\")\n",
        "print(\"\\n The accuracy for Linear regression is: \"+str(lre.evaluate(lrp)))\n",
        "\n",
        "#Decision tree classifier\n",
        "dtp = dtm.transform(test)\n",
        "dte = RegressionEvaluator(\n",
        "labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = dte.evaluate(dt_predictions)\n",
        "print(\"\\n The accuracy for Decision tree classifier is: \"+str(r2))\n",
        "\n",
        "#Gradient booster\n",
        "gbtp = gbtm.transform(test)\n",
        "gbte = RegressionEvaluator(\n",
        "labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r2 = gbte.evaluate(gbtp)\n",
        "print(\"\\n The accuracy for Gradient Booster is: \"+str(r2))\n",
        "\n"
      ],
      "metadata": {
        "id": "lXlZS0ZncJP1",
        "outputId": "c1f664f6-d9bf-4e6d-b563-14fec41d3057",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Correlation corresponding to last AQI column: \n",
            "[0.91519013 0.90473294 0.47765931 0.54618293 0.58023652 0.34522906\n",
            " 0.60818697 0.18335878 0.09487005 0.14251333 0.21557067 0.14224832\n",
            " 1.        ]\n",
            "\n",
            " AirQuality columns: \n",
            "['PM25', 'PM10', 'NO', 'NO2', 'NOX', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']\n",
            "\n",
            " The filtered features and aqi columns: \n",
            "+--------------------+-----+\n",
            "|            features|  aqi|\n",
            "+--------------------+-----+\n",
            "|[81.4,124.5,20.5,...|184.0|\n",
            "|[78.32,129.06,26....|197.0|\n",
            "|[88.76,135.32,30....|198.0|\n",
            "|[64.18,104.09,28....|188.0|\n",
            "|[72.47,114.84,23....|173.0|\n",
            "|[69.8,114.86,20.1...|165.0|\n",
            "|[73.96,113.56,19....|191.0|\n",
            "|[89.9,140.2,26.19...|191.0|\n",
            "|[87.14,130.52,21....|227.0|\n",
            "|[84.64,125.0,26.9...|168.0|\n",
            "|[88.36,121.77,20....|198.0|\n",
            "|[96.83,139.36,25....|201.0|\n",
            "|[117.46,181.64,41...|252.0|\n",
            "|[122.88,208.86,54...|310.0|\n",
            "|[74.28,141.22,44....|196.0|\n",
            "|[50.32,102.77,33....|132.0|\n",
            "|[58.47,115.27,41....|147.0|\n",
            "|[89.35,131.48,42....|179.0|\n",
            "|[64.42,99.74,34.7...|145.0|\n",
            "|[69.4,98.94,29.97...|115.0|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            " The Train and Test count respectively 7154 3160\n",
            "\n",
            " Coefficients: [0.8538756728470448,0.47334613118622537,-0.07410439359273882,0.21108059691279882,0.03250588335814234]\n",
            " Intercept: 19.436721893681646\n",
            "\n",
            "+------------------+----+--------------------+\n",
            "|        prediction| aqi|            features|\n",
            "+------------------+----+--------------------+\n",
            "| 26.92352654731893|21.0|[1.31,10.83,11.38...|\n",
            "|31.320427397085435|66.0|[2.65,17.79,11.12...|\n",
            "|30.643105656836383|21.0|[3.51,15.13,10.12...|\n",
            "| 26.20288299047766|27.0|[3.76,5.77,8.07,6...|\n",
            "| 33.12565586238769|24.0|[4.01,19.04,11.76...|\n",
            "+------------------+----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            " The accuracy for Linear regression is: 0.884375929071907\n",
            "\n",
            " The accuracy for Decision tree classifier is: 0.8872580074911625\n",
            "\n",
            " The accuracy for Gradient Booster is: 0.8941650587733869\n"
          ]
        }
      ]
    }
  ]
}